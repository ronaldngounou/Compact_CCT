{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import  pandas as pd\n",
        "\n",
        "class DataSet:\n",
        "    \"\"\"\n",
        "    This class is used to create the data to be used in the CCT model\n",
        "    \"\"\"\n",
        "    def __init__(self, fp='C:\\Education\\MSCS UDC\\Research Projects\\CCT Project\\datasets\\Ammonium ACTSOR-AQUA-UDC-1 2017-01-26T01%3A01&to=2017-01-26T12%3A59.json', look_back=5) -> None:\n",
        "        self.fp = fp # file path\n",
        "        self.data = self.load_file() \n",
        "        self.look_back = look_back\n",
        "        self.dataset = self.__data() \n",
        "        self.X, self.y = self.create_dataset()\n",
        "\n",
        "    def load_file(self):\n",
        "        data = pd.read_json(self.fp)\n",
        "        data['readTime'] = pd.to_datetime(data['readTime'])\n",
        "        data.set_index('readTime', inplace=True)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __data(self):\n",
        "        \"\"\"\n",
        "        Perfom min-max scaling on the data\n",
        "        \"\"\"\n",
        "        dataset = self.data\n",
        "        max_value = np.max(dataset['ammonium']) \n",
        "        min_value = np.min(dataset['ammonium']) \n",
        "        range_val = max_value - min_value\n",
        "        return list(map(lambda x: x/range_val, dataset['ammonium']))\n",
        "\n",
        "    def create_dataset(self):\n",
        "        X, y = [], []\n",
        "        for i in range(len(self.dataset) - self.look_back): # create sequences with labels.\n",
        "            a = self.dataset[i:(i+self.look_back)]\n",
        "            X.append(a)\n",
        "            y.append(self.dataset[i+self.look_back])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def perfom_splits(self):\n",
        "        \"\"\"\n",
        "        Create a training and testing split on the data\n",
        "        \"\"\"\n",
        "        train_size = int(len(self.X)*0.9)\n",
        "        test_size = len(self.X) - train_size\n",
        "        x_train, y_train = self.X[:train_size], self.y[:train_size]\n",
        "        x_test, y_test = self.X[train_size:], self.y[train_size:]\n",
        "        x_train,x_test = torch.from_numpy(x_train.reshape(-1, self.look_back, 1)), \\\n",
        "            torch.from_numpy(x_test.reshape(-1, self.look_back, 1)) \n",
        "        y_train, y_test = torch.from_numpy(y_train.reshape(-1, 1, 1)), \\\n",
        "            torch.from_numpy(y_test.reshape(-1, 1, 1))\n",
        "\n",
        "        assert y_train.size()[0] == x_train.size()[0]\n",
        "        assert y_train.size()[1:] == y_test.size()[1:]\n",
        "        assert x_train.size()[1:] == x_test.size()[1:]\n",
        "        assert y_test.size()[0] == x_test.size()[0]\n",
        "        return x_train,x_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvBNevl0N7DN",
        "outputId": "c2c74ecd-28b4-405e-c4df-59f1fcc444ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (0.9.12)\n",
            "Requirement already satisfied: torch>=1.7 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from timm) (2.1.2)\n",
            "Requirement already satisfied: torchvision in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from timm) (0.16.2)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from timm) (0.20.1)\n",
            "Requirement already satisfied: safetensors in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from torch>=1.7->timm) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from torch>=1.7->timm) (2023.12.2)\n",
            "Requirement already satisfied: requests in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from torchvision->timm) (10.0.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->timm) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from jinja2->torch>=1.7->timm) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from requests->huggingface-hub->timm) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\landry\\anaconda3\\envs\\cct\\lib\\site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:151: UserWarning: Overwriting cct_2_3x2_32 in registry with __main__.cct_2_3x2_32. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_2_3x2_32(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:161: UserWarning: Overwriting cct_2_3x2_32_sine in registry with __main__.cct_2_3x2_32_sine. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_2_3x2_32_sine(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:172: UserWarning: Overwriting cct_4_3x2_32 in registry with __main__.cct_4_3x2_32. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_4_3x2_32(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:183: UserWarning: Overwriting cct_4_3x2_32_sine in registry with __main__.cct_4_3x2_32_sine. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_4_3x2_32_sine(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:194: UserWarning: Overwriting cct_6_3x1_32 in registry with __main__.cct_6_3x1_32. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_6_3x1_32(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:205: UserWarning: Overwriting cct_6_3x1_32_sine in registry with __main__.cct_6_3x1_32_sine. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_6_3x1_32_sine(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:216: UserWarning: Overwriting cct_6_3x2_32 in registry with __main__.cct_6_3x2_32. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_6_3x2_32(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:227: UserWarning: Overwriting cct_6_3x2_32_sine in registry with __main__.cct_6_3x2_32_sine. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_6_3x2_32_sine(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:238: UserWarning: Overwriting cct_7_3x1_32 in registry with __main__.cct_7_3x1_32. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_7_3x1_32(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:249: UserWarning: Overwriting cct_7_3x1_32_sine in registry with __main__.cct_7_3x1_32_sine. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_7_3x1_32_sine(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:260: UserWarning: Overwriting cct_7_3x1_32_c100 in registry with __main__.cct_7_3x1_32_c100. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_7_3x1_32_c100(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:271: UserWarning: Overwriting cct_7_3x1_32_sine_c100 in registry with __main__.cct_7_3x1_32_sine_c100. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_7_3x1_32_sine_c100(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:282: UserWarning: Overwriting cct_7_3x2_32 in registry with __main__.cct_7_3x2_32. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_7_3x2_32(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:293: UserWarning: Overwriting cct_7_3x2_32_sine in registry with __main__.cct_7_3x2_32_sine. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_7_3x2_32_sine(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:304: UserWarning: Overwriting cct_7_7x2_224 in registry with __main__.cct_7_7x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_7_7x2_224(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:315: UserWarning: Overwriting cct_7_7x2_224_sine in registry with __main__.cct_7_7x2_224_sine. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_7_7x2_224_sine(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:326: UserWarning: Overwriting cct_14_7x2_224 in registry with __main__.cct_14_7x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_14_7x2_224(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:337: UserWarning: Overwriting cct_14_7x2_384 in registry with __main__.cct_14_7x2_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_14_7x2_384(pretrained=False, progress=False,\n",
            "C:\\Users\\Landry\\AppData\\Local\\Temp\\ipykernel_7440\\1678359253.py:348: UserWarning: Overwriting cct_14_7x2_384_fl in registry with __main__.cct_14_7x2_384_fl. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def cct_14_7x2_384_fl(pretrained=False, progress=False,\n"
          ]
        }
      ],
      "source": [
        "#!unzip utils\n",
        "!pip install timm\n",
        "\n",
        "# Original Code from the SHI-Labs\n",
        "# .utils in the from lines changed to utils\n",
        "# .registry import changed to the function def (copied from the registry.py)\n",
        "\n",
        "from torch.hub import load_state_dict_from_url\n",
        "import torch.nn as nn\n",
        "from utils.transformers import TransformerClassifier\n",
        "from utils.tokenizer import Tokenizer\n",
        "from utils.helpers import pe_check, fc_check\n",
        "\n",
        "try:\n",
        "    from timm.models.registry import register_model\n",
        "except ImportError:\n",
        "    #from .registry import register_model\n",
        "    def register_model(func):\n",
        "      \"\"\"\n",
        "      Fallback wrapper in case timm isn't installed\n",
        "      \"\"\"\n",
        "      return func\n",
        "\n",
        "model_urls = {\n",
        "    'cct_7_3x1_32':\n",
        "        'https://shi-labs.com/projects/cct/checkpoints/pretrained/cct_7_3x1_32_cifar10_300epochs.pth',\n",
        "    'cct_7_3x1_32_sine':\n",
        "        'https://shi-labs.com/projects/cct/checkpoints/pretrained/cct_7_3x1_32_sine_cifar10_5000epochs.pth',\n",
        "    'cct_7_3x1_32_c100':\n",
        "        'https://shi-labs.com/projects/cct/checkpoints/pretrained/cct_7_3x1_32_cifar100_300epochs.pth',\n",
        "    'cct_7_3x1_32_sine_c100':\n",
        "        'https://shi-labs.com/projects/cct/checkpoints/pretrained/cct_7_3x1_32_sine_cifar100_5000epochs.pth',\n",
        "    'cct_7_7x2_224_sine':\n",
        "        'https://shi-labs.com/projects/cct/checkpoints/pretrained/cct_7_7x2_224_flowers102.pth',\n",
        "    'cct_14_7x2_224':\n",
        "        'https://shi-labs.com/projects/cct/checkpoints/pretrained/cct_14_7x2_224_imagenet.pth',\n",
        "    'cct_14_7x2_384':\n",
        "        'https://shi-labs.com/projects/cct/checkpoints/finetuned/cct_14_7x2_384_imagenet.pth',\n",
        "    'cct_14_7x2_384_fl':\n",
        "        'https://shi-labs.com/projects/cct/checkpoints/finetuned/cct_14_7x2_384_flowers102.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class CCT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 sequence_length = 5,\n",
        "                 embedding_dim=128, # change the value\n",
        "                 n_input_channels=1, #Two channels for time and data\n",
        "                 n_conv_layers=1,\n",
        "                 kernel_size=1,\n",
        "                 stride=1, #slide the kernel over 1 position\n",
        "                 padding=0,\n",
        "                 dropout=0.,\n",
        "                 attention_dropout=0.1,\n",
        "                 stochastic_depth=0.1,\n",
        "                 num_layers=14,\n",
        "                 num_heads=4,\n",
        "                 mlp_ratio=4.0,\n",
        "                 num_classes=1,\n",
        "                 positional_embedding='learnable',\n",
        "                 *args, **kwargs):\n",
        "        super(CCT, self).__init__()\n",
        "\n",
        "        self.tokenizer = Tokenizer(n_input_channels=self.look_back,\n",
        "                                   n_output_channels=embedding_dim,\n",
        "                                   kernel_size=kernel_size,\n",
        "                                   stride=stride,\n",
        "                                   padding=padding,\n",
        "                                   activation=nn.ReLU,\n",
        "                                   n_conv_layers=n_conv_layers,\n",
        "                                   conv_bias=False)\n",
        "\n",
        "        self.classifier = TransformerClassifier(\n",
        "            sequence_length=sequence_length,\n",
        "            embedding_dim=embedding_dim,\n",
        "            seq_pool=True,\n",
        "            dropout=dropout,\n",
        "            attention_dropout=attention_dropout,\n",
        "            stochastic_depth=stochastic_depth,\n",
        "            num_layers=num_layers,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            num_classes=num_classes,\n",
        "            positional_embedding=positional_embedding\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenizer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def _cct(arch, pretrained, progress,\n",
        "         num_layers, num_heads, mlp_ratio, embedding_dim,\n",
        "         kernel_size=3, stride=None, padding=None,\n",
        "         positional_embedding='learnable',\n",
        "         *args, **kwargs):\n",
        "    stride = stride if stride is not None else max(1, (kernel_size // 2) - 1)\n",
        "    padding = padding if padding is not None else max(1, (kernel_size // 2))\n",
        "    model = CCT(num_layers=num_layers,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                embedding_dim=embedding_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                *args, **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        if arch in model_urls:\n",
        "            state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                                  progress=progress)\n",
        "            if positional_embedding == 'learnable':\n",
        "                state_dict = pe_check(model, state_dict)\n",
        "            elif positional_embedding == 'sine':\n",
        "                state_dict['classifier.positional_emb'] = model.state_dict()['classifier.positional_emb']\n",
        "            state_dict = fc_check(model, state_dict)\n",
        "            model.load_state_dict(state_dict)\n",
        "        else:\n",
        "            raise RuntimeError(f'Variant {arch} does not yet have pretrained weights.')\n",
        "    return model\n",
        "\n",
        "# I don't need anything after all this.\n",
        "\n",
        "def cct_2(arch, pretrained, progress, *args, **kwargs):\n",
        "    return _cct(arch, pretrained, progress, num_layers=2, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_4(arch, pretrained, progress, *args, **kwargs):\n",
        "    return _cct(arch, pretrained, progress, num_layers=4, num_heads=2, mlp_ratio=1, embedding_dim=128,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_6(arch, pretrained, progress, *args, **kwargs):\n",
        "    return _cct(arch, pretrained, progress, num_layers=6, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_7(arch, pretrained, progress, *args, **kwargs):\n",
        "    return _cct(arch, pretrained, progress, num_layers=7, num_heads=4, mlp_ratio=2, embedding_dim=256,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "def cct_14(arch, pretrained, progress, *args, **kwargs):\n",
        "    return _cct(arch, pretrained, progress, num_layers=14, num_heads=6, mlp_ratio=3, embedding_dim=384,\n",
        "                *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_2_3x2_32(pretrained=False, progress=False,\n",
        "                 img_size=32, positional_embedding='learnable',\n",
        "                 *args, **kwargs):\n",
        "    return cct_2('cct_2_3x2_32', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_2_3x2_32_sine(pretrained=False, progress=False,\n",
        "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
        "                      *args, **kwargs):\n",
        "    return cct_2('cct_2_3x2_32_sine', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_4_3x2_32(pretrained=False, progress=False,\n",
        "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
        "                 *args, **kwargs):\n",
        "    return cct_4('cct_4_3x2_32', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_4_3x2_32_sine(pretrained=False, progress=False,\n",
        "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
        "                      *args, **kwargs):\n",
        "    return cct_4('cct_4_3x2_32_sine', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_6_3x1_32(pretrained=False, progress=False,\n",
        "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
        "                 *args, **kwargs):\n",
        "    return cct_6('cct_6_3x1_32', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=1,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_6_3x1_32_sine(pretrained=False, progress=False,\n",
        "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
        "                      *args, **kwargs):\n",
        "    return cct_6('cct_6_3x1_32_sine', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=1,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_6_3x2_32(pretrained=False, progress=False,\n",
        "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
        "                 *args, **kwargs):\n",
        "    return cct_6('cct_6_3x2_32', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_6_3x2_32_sine(pretrained=False, progress=False,\n",
        "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
        "                      *args, **kwargs):\n",
        "    return cct_6('cct_6_3x2_32_sine', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_7_3x1_32(pretrained=False, progress=False,\n",
        "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
        "                 *args, **kwargs):\n",
        "    return cct_7('cct_7_3x1_32', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=1,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_7_3x1_32_sine(pretrained=False, progress=False,\n",
        "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
        "                      *args, **kwargs):\n",
        "    return cct_7('cct_7_3x1_32_sine', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=1,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_7_3x1_32_c100(pretrained=False, progress=False,\n",
        "                      img_size=32, positional_embedding='learnable', num_classes=100,\n",
        "                      *args, **kwargs):\n",
        "    return cct_7('cct_7_3x1_32_c100', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=1,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_7_3x1_32_sine_c100(pretrained=False, progress=False,\n",
        "                           img_size=32, positional_embedding='sine', num_classes=100,\n",
        "                           *args, **kwargs):\n",
        "    return cct_7('cct_7_3x1_32_sine_c100', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=1,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_7_3x2_32(pretrained=False, progress=False,\n",
        "                 img_size=32, positional_embedding='learnable', num_classes=10,\n",
        "                 *args, **kwargs):\n",
        "    return cct_7('cct_7_3x2_32', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_7_3x2_32_sine(pretrained=False, progress=False,\n",
        "                      img_size=32, positional_embedding='sine', num_classes=10,\n",
        "                      *args, **kwargs):\n",
        "    return cct_7('cct_7_3x2_32_sine', pretrained, progress,\n",
        "                 kernel_size=3, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_7_7x2_224(pretrained=False, progress=False,\n",
        "                  img_size=224, positional_embedding='learnable', num_classes=102,\n",
        "                  *args, **kwargs):\n",
        "    return cct_7('cct_7_7x2_224', pretrained, progress,\n",
        "                 kernel_size=7, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_7_7x2_224_sine(pretrained=False, progress=False,\n",
        "                       img_size=224, positional_embedding='sine', num_classes=102,\n",
        "                       *args, **kwargs):\n",
        "    return cct_7('cct_7_7x2_224_sine', pretrained, progress,\n",
        "                 kernel_size=7, n_conv_layers=2,\n",
        "                 img_size=img_size, positional_embedding=positional_embedding,\n",
        "                 num_classes=num_classes,\n",
        "                 *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_14_7x2_224(pretrained=False, progress=False,\n",
        "                   img_size=224, positional_embedding='learnable', num_classes=1000,\n",
        "                   *args, **kwargs):\n",
        "    return cct_14('cct_14_7x2_224', pretrained, progress,\n",
        "                  kernel_size=7, n_conv_layers=2,\n",
        "                  img_size=img_size, positional_embedding=positional_embedding,\n",
        "                  num_classes=num_classes,\n",
        "                  *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_14_7x2_384(pretrained=False, progress=False,\n",
        "                   img_size=384, positional_embedding='learnable', num_classes=1000,\n",
        "                   *args, **kwargs):\n",
        "    return cct_14('cct_14_7x2_384', pretrained, progress,\n",
        "                  kernel_size=7, n_conv_layers=2,\n",
        "                  img_size=img_size, positional_embedding=positional_embedding,\n",
        "                  num_classes=num_classes,\n",
        "                  *args, **kwargs)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def cct_14_7x2_384_fl(pretrained=False, progress=False,\n",
        "                      img_size=384, positional_embedding='learnable', num_classes=102,\n",
        "                      *args, **kwargs):\n",
        "    return cct_14('cct_14_7x2_384_fl', pretrained, progress,\n",
        "                  kernel_size=7, n_conv_layers=2,\n",
        "                  img_size=img_size, positional_embedding=positional_embedding,\n",
        "                  num_classes=num_classes,\n",
        "                  *args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_steps = 1\n",
        "dataset = DataSet()\n",
        "data_X, data_Y = dataset.X, dataset.y\n",
        "\n",
        "x_train,x_test, y_train, y_test = dataset.perfom_splits()\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "#x_train = np.reshape(x_train, (len(x_train), time_steps, 1))  \n",
        "#x_test = np.reshape(x_test, (len(x_test), time_steps, 1)) #Don't need to have a timesteps because that is where the issue might have come from before.\n",
        "# print(x_train)\n",
        "#print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "x_train_tensor = torch.Tensor(x_train)\n",
        "y_train_tensor = torch.Tensor(y_train)\n",
        "x_test_tensor = torch.Tensor(x_test)\n",
        "y_test_tensor = torch.Tensor(y_test)\n",
        "\n",
        "# Create PyTorch TensorDatasets\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 5, 1])\n",
            "Shape of y: torch.Size([64, 1, 1]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "for X, y in train_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 5, 1])\n",
            "Shape of y: torch.Size([64, 1, 1]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(614, 5, 1)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'CCT' object has no attribute 'look_back'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_cct \u001b[38;5;241m=\u001b[39m \u001b[43mCCT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_cct)\n",
            "Cell \u001b[1;32mIn[23], line 64\u001b[0m, in \u001b[0;36mCCT.__init__\u001b[1;34m(self, sequence_length, embedding_dim, n_input_channels, n_conv_layers, kernel_size, stride, padding, dropout, attention_dropout, stochastic_depth, num_layers, num_heads, mlp_ratio, num_classes, positional_embedding, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m              sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     47\u001b[0m              embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, \u001b[38;5;66;03m# change the value\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m              positional_embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearnable\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     61\u001b[0m              \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28msuper\u001b[39m(CCT, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(n_input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlook_back\u001b[49m,\n\u001b[0;32m     65\u001b[0m                                n_output_channels\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[0;32m     66\u001b[0m                                kernel_size\u001b[38;5;241m=\u001b[39mkernel_size,\n\u001b[0;32m     67\u001b[0m                                stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m     68\u001b[0m                                padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m     69\u001b[0m                                activation\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU,\n\u001b[0;32m     70\u001b[0m                                n_conv_layers\u001b[38;5;241m=\u001b[39mn_conv_layers,\n\u001b[0;32m     71\u001b[0m                                conv_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m TransformerClassifier(\n\u001b[0;32m     74\u001b[0m         sequence_length\u001b[38;5;241m=\u001b[39msequence_length,\n\u001b[0;32m     75\u001b[0m         embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m         positional_embedding\u001b[38;5;241m=\u001b[39mpositional_embedding\n\u001b[0;32m     85\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'CCT' object has no attribute 'look_back'"
          ]
        }
      ],
      "source": [
        "model_cct = CCT()\n",
        "print(model_cct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [128, 1, 1], expected input[64, 5, 1] to have 1 channels, but got 5 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass with input data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[13], line 88\u001b[0m, in \u001b[0;36mCCT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 88\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Education\\MSCS UDC\\Research Projects\\CCT Project\\utils\\tokenizer.py:39\u001b[0m, in \u001b[0;36mTokenizer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 1, 1], expected input[64, 5, 1] to have 1 channels, but got 5 channels instead"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim \n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(model_cct.parameters(), lr= 0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model_cct.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for X, y in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with input data\n",
        "        output = model_cct(X)\n",
        "        print(output.shape)\n",
        "        print(y.shape)\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.squeeze(), y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print or log average training loss for the epoch\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}')\n",
        "\n",
        "# Optionally, you can evaluate the model on the test set after training\n",
        "model_cct.eval()\n",
        "with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "        output = model_cct(X)\n",
        "        test_loss = criterion(output.squeeze(), y)\n",
        "\n",
        "    # Print or log test loss\n",
        "    print(f'Test Loss: {test_loss.item()}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model_cct' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Modified to separate loss function and optimizer for each model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m loss_fn_cct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 4\u001b[0m optimizer_cct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel_cct\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model_cct' is not defined"
          ]
        }
      ],
      "source": [
        "#Modified to separate loss function and optimizer for each model\n",
        "\n",
        "loss_fn_cct = nn.CrossEntropyLoss()\n",
        "optimizer_cct = torch.optim.SGD(model_cct.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CCT Model: Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [128, 1, 3], expected input[64, 5, 1] to have 1 channels, but got 5 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCCT Model: Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCCT Done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[2], line 88\u001b[0m, in \u001b[0;36mCCT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 88\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Education\\MSCS UDC\\Research Projects\\CCT Project\\utils\\tokenizer.py:39\u001b[0m, in \u001b[0;36mTokenizer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Landry\\anaconda3\\envs\\CCT\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 1, 3], expected input[64, 5, 1] to have 1 channels, but got 5 channels instead"
          ]
        }
      ],
      "source": [
        "#Modified: train&test the CCT model\n",
        "epochs = 5\n",
        "model = model_cct\n",
        "loss_fn = loss_fn_cct\n",
        "optimizer = optimizer_cct\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"CCT Model: Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"CCT Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsSNrc2NL9JN"
      },
      "source": [
        "\n",
        "[Learn the Basics](intro.html) ||\n",
        "**Quickstart** ||\n",
        "[Tensors](tensorqs_tutorial.html) ||\n",
        "[Datasets & DataLoaders](data_tutorial.html) ||\n",
        "[Transforms](transforms_tutorial.html) ||\n",
        "[Build Model](buildmodel_tutorial.html) ||\n",
        "[Autograd](autogradqs_tutorial.html) ||\n",
        "[Optimization](optimization_tutorial.html) ||\n",
        "[Save & Load Model](saveloadrun_tutorial.html)\n",
        "\n",
        "# Quickstart\n",
        "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\n",
        "\n",
        "## Working with data\n",
        "PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html):\n",
        "``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n",
        "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
        "the ``Dataset``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr3jvetOL9JP"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'ToTensor' from 'torchvision.transforms' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'ToTensor' from 'torchvision.transforms' (unknown location)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJdFuOnL9JP"
      },
      "source": [
        "PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html),\n",
        "all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\n",
        "\n",
        "The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n",
        "CIFAR, COCO ([full list here](https://pytorch.org/vision/stable/datasets.html)). In this tutorial, we\n",
        "use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n",
        "``target_transform`` to modify the samples and labels respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "221bfcc5d988451e87219c9a125fbaa0",
            "40dcb1822bde4c9dacd8a1479dec0865",
            "25982799fca743fdb9f284fd2cd0cc27",
            "5e302101670049008e1e8cd199218102",
            "a27be89e2b874bb3a5ddad4e8d8254b1",
            "f06f0d24b79d41d6bb3f990aefffc2ca",
            "98136b2293ee45e785dfa3635ae98f3e",
            "61839386cdce4c008e6d81021212eb90",
            "903816d0f940404d9bfaeca1febc2b16",
            "0e19ecce4c4e49df8772eea8c8091caf",
            "24fd6bba765b4440941eb3835e455bc8"
          ]
        },
        "id": "HCd_6kgeL9JP",
        "outputId": "7b29be75-41e6-42b1-9d24-b4fb1c3eefee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 170498071/170498071 [00:30<00:00, 5594503.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# This cell code changed from FashionMNIST to CIFAR10\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLzUOXQeL9JQ"
      },
      "source": [
        "We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n",
        "automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n",
        "in the dataloader iterable will return a batch of 64 features and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LQGMsSAL9JQ",
        "outputId": "ff1a4230-a213-4a5b-8bd2-d24c9cff0ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 3, 32, 32])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPe7i8KdL9JQ"
      },
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_57OX3vL9JR"
      },
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObUBi80yL9JR"
      },
      "source": [
        "## Creating Models\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network\n",
        "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
        "operations in the neural network, we move it to the GPU if available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9abAhmhL9JR",
        "outputId": "a47c3558-e598-4e1f-bd49-b848b964053e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# The first linear layer input changed from 28*28 to 32*32*3 (due to FashionNNIST to CIFAR10)\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(32*32*3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model_linear = NeuralNetwork().to(device)\n",
        "print(model_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzlKqSynwvTS"
      },
      "source": [
        "## CCT Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD1KOvJFwyXQ",
        "outputId": "ba2642f6-9d67-4cea-b181-9b834d101188"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# The first linear layer input changed from 28*28 to 32*32*3 (due to FashionNNIST to CIFAR10)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Get cpu or gpu device for training.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m model_cct \u001b[38;5;241m=\u001b[39m cct_2_3x2_32()\u001b[38;5;241m.\u001b[39mto(device)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# The first linear layer input changed from 28*28 to 32*32*3 (due to FashionNNIST to CIFAR10)\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model_cct = cct_2_3x2_32().to(device)\n",
        "print(model_cct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aR_QVG3L9JS"
      },
      "source": [
        "Read more about [building neural networks in PyTorch](buildmodel_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un70wrwjL9JS"
      },
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibq_YVHkL9JS"
      },
      "source": [
        "## Optimizing the Model Parameters\n",
        "To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "and an [optimizer](https://pytorch.org/docs/stable/optim.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hXeWCVFL9JS"
      },
      "outputs": [],
      "source": [
        "#Modified to separate loss function and optimizer for each model\n",
        "\n",
        "loss_fn_linear = nn.CrossEntropyLoss()\n",
        "optimizer_linear = torch.optim.SGD(model_linear.parameters(), lr=1e-3)\n",
        "\n",
        "loss_fn_cct = nn.CrossEntropyLoss()\n",
        "optimizer_cct = torch.optim.SGD(model_cct.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsSH1wpnL9JS"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
        "backpropagates the prediction error to adjust the model's parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzNIXSrIL9JS"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBwBmxbvL9JT"
      },
      "source": [
        "We also check the model's performance against the test dataset to ensure it is learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5SdXu7YL9JT"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7TmZIlHL9JT"
      },
      "source": [
        "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
        "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
        "accuracy increase and the loss decrease with every epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oRZ9ieaL9JT",
        "outputId": "c85fad9c-4d52-4630-dedd-933d1747f924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Model: Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300155  [    0/50000]\n",
            "loss: 2.301221  [ 6400/50000]\n",
            "loss: 2.292411  [12800/50000]\n",
            "loss: 2.283465  [19200/50000]\n",
            "loss: 2.281371  [25600/50000]\n",
            "loss: 2.273720  [32000/50000]\n",
            "loss: 2.275151  [38400/50000]\n",
            "loss: 2.280617  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 16.7%, Avg loss: 2.265975 \n",
            "\n",
            "Linear Model: Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.279969  [    0/50000]\n",
            "loss: 2.278288  [ 6400/50000]\n",
            "loss: 2.239245  [12800/50000]\n",
            "loss: 2.263614  [19200/50000]\n",
            "loss: 2.238737  [25600/50000]\n",
            "loss: 2.233803  [32000/50000]\n",
            "loss: 2.248610  [38400/50000]\n",
            "loss: 2.239021  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 20.9%, Avg loss: 2.225428 \n",
            "\n",
            "Linear Model: Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.247787  [    0/50000]\n",
            "loss: 2.241838  [ 6400/50000]\n",
            "loss: 2.177146  [12800/50000]\n",
            "loss: 2.233646  [19200/50000]\n",
            "loss: 2.185372  [25600/50000]\n",
            "loss: 2.183100  [32000/50000]\n",
            "loss: 2.220221  [38400/50000]\n",
            "loss: 2.182153  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 23.2%, Avg loss: 2.172330 \n",
            "\n",
            "Linear Model: Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.206738  [    0/50000]\n",
            "loss: 2.194497  [ 6400/50000]\n",
            "loss: 2.099627  [12800/50000]\n",
            "loss: 2.193623  [19200/50000]\n",
            "loss: 2.125990  [25600/50000]\n",
            "loss: 2.124031  [32000/50000]\n",
            "loss: 2.196407  [38400/50000]\n",
            "loss: 2.118811  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 24.9%, Avg loss: 2.116741 \n",
            "\n",
            "Linear Model: Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.164539  [    0/50000]\n",
            "loss: 2.146683  [ 6400/50000]\n",
            "loss: 2.022498  [12800/50000]\n",
            "loss: 2.152546  [19200/50000]\n",
            "loss: 2.076212  [25600/50000]\n",
            "loss: 2.073316  [32000/50000]\n",
            "loss: 2.180588  [38400/50000]\n",
            "loss: 2.065917  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 26.7%, Avg loss: 2.070174 \n",
            "\n",
            "Linear Done!\n",
            "CPU times: user 1min 42s, sys: 239 ms, total: 1min 42s\n",
            "Wall time: 1min 42s\n"
          ]
        }
      ],
      "source": [
        "#Modified: train&test the linear model\n",
        "%%time\n",
        "epochs = 5\n",
        "model = model_linear\n",
        "loss_fn = loss_fn_linear\n",
        "optimizer = optimizer_linear\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Linear Model: Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Linear Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8uVvujbkR__",
        "outputId": "3d759b31-1442-410b-e148-901957677538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved the linear model state to model_linear.pth\n",
            "Saved CCT model state to model_cct.pth\n"
          ]
        }
      ],
      "source": [
        "#Saving the linear model\n",
        "\n",
        "torch.save(model_linear.state_dict(), \"model_linear.pth\")\n",
        "print(\"Saved the linear model state to model_linear.pth\")\n",
        "\n",
        "torch.save(model_cct.state_dict(), \"model_cct.pth\")\n",
        "print(\"Saved CCT model state to model_cct.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yC9DAXcdj6y",
        "outputId": "1ce91919-c54b-481d-fb71-0cbdf584525a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CCT Model: Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.294594  [    0/50000]\n",
            "loss: 2.299885  [ 6400/50000]\n",
            "loss: 2.291082  [12800/50000]\n",
            "loss: 2.301929  [19200/50000]\n",
            "loss: 2.289249  [25600/50000]\n",
            "loss: 2.271498  [32000/50000]\n",
            "loss: 2.271566  [38400/50000]\n",
            "loss: 2.268686  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 19.6%, Avg loss: 2.258488 \n",
            "\n",
            "CCT Model: Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.274891  [    0/50000]\n",
            "loss: 2.256397  [ 6400/50000]\n",
            "loss: 2.221311  [12800/50000]\n",
            "loss: 2.254772  [19200/50000]\n",
            "loss: 2.250368  [25600/50000]\n",
            "loss: 2.204877  [32000/50000]\n",
            "loss: 2.201191  [38400/50000]\n",
            "loss: 2.206580  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 22.1%, Avg loss: 2.184597 \n",
            "\n",
            "CCT Model: Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.218835  [    0/50000]\n",
            "loss: 2.187050  [ 6400/50000]\n",
            "loss: 2.113835  [12800/50000]\n",
            "loss: 2.181562  [19200/50000]\n",
            "loss: 2.195395  [25600/50000]\n",
            "loss: 2.106834  [32000/50000]\n",
            "loss: 2.098742  [38400/50000]\n",
            "loss: 2.109198  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 24.4%, Avg loss: 2.084748 \n",
            "\n",
            "CCT Model: Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.142495  [    0/50000]\n",
            "loss: 2.107205  [ 6400/50000]\n",
            "loss: 2.000244  [12800/50000]\n",
            "loss: 2.104309  [19200/50000]\n",
            "loss: 2.161287  [25600/50000]\n",
            "loss: 2.027866  [32000/50000]\n",
            "loss: 2.031875  [38400/50000]\n",
            "loss: 2.031961  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 26.1%, Avg loss: 2.020806 \n",
            "\n",
            "CCT Model: Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.086511  [    0/50000]\n",
            "loss: 2.057445  [ 6400/50000]\n",
            "loss: 1.938670  [12800/50000]\n",
            "loss: 2.050220  [19200/50000]\n",
            "loss: 2.143164  [25600/50000]\n",
            "loss: 1.980496  [32000/50000]\n",
            "loss: 1.998909  [38400/50000]\n",
            "loss: 1.982376  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 27.6%, Avg loss: 1.980738 \n",
            "\n",
            "CCT Done!\n",
            "CPU times: user 26min 48s, sys: 5.2 s, total: 26min 53s\n",
            "Wall time: 27min 1s\n"
          ]
        }
      ],
      "source": [
        "#Modified: train&test the CCT model\n",
        "%%time\n",
        "epochs = 5\n",
        "model = model_cct\n",
        "loss_fn = loss_fn_cct\n",
        "optimizer = optimizer_cct\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"CCT Model: Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"CCT Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v57-e0MYL9JT"
      },
      "source": [
        "Read more about [Training your model](optimization_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7pKGeA6kU0e",
        "outputId": "f9abaaf9-1f8b-4862-a3ed-64396e03b8e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved CCT model state to model_cct.pth\n"
          ]
        }
      ],
      "source": [
        "#Saving the CCT model\n",
        "\n",
        "torch.save(model_cct.state_dict(), \"model_cct.pth\")\n",
        "print(\"Saved CCT model state to model_cct.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxvhF3T-L9JT"
      },
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip3CHEp3L9JU"
      },
      "source": [
        "## Loading and Using the Models\n",
        "\n",
        "The process for loading a model includes re-creating the model structure and loading\n",
        "the state dictionary into it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwxOaFDeZ0u8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_images(x, y, class_names, x_adv=None, y_adv=None):\n",
        "    if x_adv is not None:\n",
        "        fig, ax = plt.subplots(3, 10, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 10, figsize=(20, 5))\n",
        "\n",
        "    for i in range(len(y)):\n",
        "        print(\"i=\",i, \"y[i]=\",y[i])\n",
        "        if x_adv is not None:\n",
        "            ax[0, i].imshow(x[i].permute(1, 2, 0))\n",
        "            ax[0, i].set_title(class_names[y[i]])\n",
        "            ax[0, i].axis(\"off\")\n",
        "            ax[1, i].imshow(x_adv[i].detach().cpu().permute(1, 2, 0))\n",
        "            if y_adv[i].item() != y[i].item():\n",
        "                ax[1, i].set_title(class_names[y_adv[i]], color=\"r\")\n",
        "            else:\n",
        "                ax[1, i].set_title(class_names[y_adv[i]], color=\"k\")\n",
        "            ax[1, i].axis(\"off\")\n",
        "            ax[2, i].imshow(\n",
        "                (torch.abs(x[i] - x_adv[i].detach().cpu()) * 25)\n",
        "                .clamp_(0, 1)\n",
        "                .permute(1, 2, 0)\n",
        "            )\n",
        "            ax[2, i].axis(\"off\")\n",
        "        else:\n",
        "            ax[i].imshow(x[i].permute(1, 2, 0))\n",
        "            ax[i].set_title(class_names[y[i]])\n",
        "            ax[i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziGqNKrgL9JU"
      },
      "outputs": [],
      "source": [
        "#Classes (Lables) changed due to FashionMNIST to CIFAR10\n",
        "'''\n",
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "'''\n",
        "classes = [\n",
        "    \"airplane\",\n",
        "    \"automobile\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwHtz8yKrqCg"
      },
      "source": [
        "## Load a model and Predict on a single input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Hp6SMuJ2rnrY",
        "outputId": "96151abd-da49-459e-8704-6ae4f54eb262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3])\n",
            "i= 0 y[i]= tensor(3)\n",
            "torch.Size([1, 3072])\n",
            "Predicted: \"dog\", Actual: \"cat\"\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x360 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# This Cell is for the Linear Model\n",
        "\n",
        "#--- Loading the Model\n",
        "model = NeuralNetwork()\n",
        "model.load_state_dict(torch.load(\"model_linear.pth\"))\n",
        "\n",
        "#--- Prediction using the loaded model\n",
        "model.eval()\n",
        "#x, y = test_data[0][0], test_data[0][1]\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "print(torch.tensor(y)[None])\n",
        "plot_images(x[None, :], list(torch.tensor(y)[None]), classes)\n",
        "x = x.view(1, -1) # Use this for the Linear Model to convert [3x32x32] to [1 x (3x32x32)] for the batch dimension\n",
        "#x=x[None, :] # Use this for the CCT Model to convert [3x32x32] to [1x3x32x32]\n",
        "print(x.shape)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "GDwKyXZlr0-J",
        "outputId": "5253addd-27ab-4b29-cea9-cfdb539daf21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3])\n",
            "i= 0 y[i]= tensor(3)\n",
            "torch.Size([1, 3, 32, 32])\n",
            "Predicted: \"automobile\", Actual: \"cat\"\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x360 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# This Cell is for the CCT Model\n",
        "\n",
        "#--- Loading the Model\n",
        "model = cct_2_3x2_32()\n",
        "model.load_state_dict(torch.load(\"model_cct.pth\"))\n",
        "\n",
        "#--- Prediction using the loaded model\n",
        "model.eval()\n",
        "#x, y = test_data[0][0], test_data[0][1]\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "print(torch.tensor(y)[None])\n",
        "plot_images(x[None, :], list(torch.tensor(y)[None]), classes)\n",
        "#x = x.view(1, -1) # Use this for the Linear Model to convert [3x32x32] to [1 x (3x32x32)] for the batch dimension\n",
        "x=x[None, :] # Use this for the CCT Model to convert [3x32x32] to [1x3x32x32]\n",
        "print(x.shape)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccElUqtvL9JU"
      },
      "source": [
        "Read more about [Saving & Loading your model](saveloadrun_tutorial.html).\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e19ecce4c4e49df8772eea8c8091caf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "221bfcc5d988451e87219c9a125fbaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40dcb1822bde4c9dacd8a1479dec0865",
              "IPY_MODEL_25982799fca743fdb9f284fd2cd0cc27",
              "IPY_MODEL_5e302101670049008e1e8cd199218102"
            ],
            "layout": "IPY_MODEL_a27be89e2b874bb3a5ddad4e8d8254b1"
          }
        },
        "24fd6bba765b4440941eb3835e455bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25982799fca743fdb9f284fd2cd0cc27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61839386cdce4c008e6d81021212eb90",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_903816d0f940404d9bfaeca1febc2b16",
            "value": 170498071
          }
        },
        "40dcb1822bde4c9dacd8a1479dec0865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f06f0d24b79d41d6bb3f990aefffc2ca",
            "placeholder": "",
            "style": "IPY_MODEL_98136b2293ee45e785dfa3635ae98f3e",
            "value": "100%"
          }
        },
        "5e302101670049008e1e8cd199218102": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e19ecce4c4e49df8772eea8c8091caf",
            "placeholder": "",
            "style": "IPY_MODEL_24fd6bba765b4440941eb3835e455bc8",
            "value": " 170498071/170498071 [00:03&lt;00:00, 54062214.16it/s]"
          }
        },
        "61839386cdce4c008e6d81021212eb90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "903816d0f940404d9bfaeca1febc2b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98136b2293ee45e785dfa3635ae98f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a27be89e2b874bb3a5ddad4e8d8254b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f06f0d24b79d41d6bb3f990aefffc2ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
